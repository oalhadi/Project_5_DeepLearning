{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade arabic-reshaper\n",
    "import arabic_reshaper\n",
    "# !pip install python-bidi\n",
    "#!pip install pyttsx3 \n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "from PIL import ImageFont, ImageDraw, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data 4')\n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['ineed', 'ambulance', 'where', 'street', 'thankyou', 'work',\n",
    "                   'bootcamp', 'in this', 'i_need_ambulance', 'i_want', 'report', 'accedint'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 10\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "# start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ineed': 0,\n",
       " 'ambulance': 1,\n",
       " 'where': 2,\n",
       " 'street': 3,\n",
       " 'thankyou': 4,\n",
       " 'work': 5,\n",
       " 'bootcamp': 6,\n",
       " 'in this': 7,\n",
       " 'i_need_ambulance': 8,\n",
       " 'i_want': 9,\n",
       " 'report': 10,\n",
       " 'accedint': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'MP_Data 4\\\\ineed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7743fde2b634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mframe_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'MP_Data 4\\\\ineed'"
     ]
    }
   ],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 30, 1662)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 30, 1662)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train GRU Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(256, return_sequences=True, activation='tanh', input_shape=(30,1662)))\n",
    "model.add(GRU(128, return_sequences=False, activation='tanh'))\n",
    "# model.add(GRU(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 30, 256)           1474560   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               148224    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                396       \n",
      "=================================================================\n",
      "Total params: 1,633,516\n",
      "Trainable params: 1,633,516\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "62/62 [==============================] - 17s 41ms/step - loss: 2.5045 - categorical_accuracy: 0.1042 - val_loss: 2.3658 - val_categorical_accuracy: 0.1558 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 2.2837 - categorical_accuracy: 0.1792 - val_loss: 2.1006 - val_categorical_accuracy: 0.2078 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 2.0218 - categorical_accuracy: 0.2182 - val_loss: 2.2896 - val_categorical_accuracy: 0.1039 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 1.8786 - categorical_accuracy: 0.2801 - val_loss: 1.7440 - val_categorical_accuracy: 0.3117 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 1.6449 - categorical_accuracy: 0.3355 - val_loss: 1.5993 - val_categorical_accuracy: 0.4675 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 1.5440 - categorical_accuracy: 0.3811 - val_loss: 1.5976 - val_categorical_accuracy: 0.3896 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 1.3468 - categorical_accuracy: 0.4886 - val_loss: 1.4286 - val_categorical_accuracy: 0.4286 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.1338 - categorical_accuracy: 0.5309 - val_loss: 0.9889 - val_categorical_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.9844 - categorical_accuracy: 0.6124 - val_loss: 0.9762 - val_categorical_accuracy: 0.6494 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.8244 - categorical_accuracy: 0.6808 - val_loss: 0.9355 - val_categorical_accuracy: 0.5844 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.8018 - categorical_accuracy: 0.6743 - val_loss: 0.6795 - val_categorical_accuracy: 0.6883 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "62/62 [==============================] - 1s 22ms/step - loss: 0.6780 - categorical_accuracy: 0.7557 - val_loss: 0.7996 - val_categorical_accuracy: 0.6623 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.6565 - categorical_accuracy: 0.7296 - val_loss: 0.4774 - val_categorical_accuracy: 0.7662 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 0.6297 - categorical_accuracy: 0.7394 - val_loss: 0.5513 - val_categorical_accuracy: 0.7532 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.5535 - categorical_accuracy: 0.7883 - val_loss: 0.5022 - val_categorical_accuracy: 0.7662 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "62/62 [==============================] - 1s 22ms/step - loss: 0.4826 - categorical_accuracy: 0.7883 - val_loss: 0.4560 - val_categorical_accuracy: 0.7662 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.4138 - categorical_accuracy: 0.8013 - val_loss: 0.5357 - val_categorical_accuracy: 0.7792 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.5565 - categorical_accuracy: 0.7655 - val_loss: 0.8104 - val_categorical_accuracy: 0.6753 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.4892 - categorical_accuracy: 0.7980\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.4892 - categorical_accuracy: 0.7980 - val_loss: 0.6839 - val_categorical_accuracy: 0.6883 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.4599 - categorical_accuracy: 0.7915 - val_loss: 0.4135 - val_categorical_accuracy: 0.7792 - lr: 5.0000e-04\n",
      "Epoch 21/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.3432 - categorical_accuracy: 0.8469 - val_loss: 0.3797 - val_categorical_accuracy: 0.8571 - lr: 5.0000e-04\n",
      "Epoch 22/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.2837 - categorical_accuracy: 0.8730 - val_loss: 0.3905 - val_categorical_accuracy: 0.7662 - lr: 5.0000e-04\n",
      "Epoch 23/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.2796 - categorical_accuracy: 0.8730 - val_loss: 0.3078 - val_categorical_accuracy: 0.8701 - lr: 5.0000e-04\n",
      "Epoch 24/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.2614 - categorical_accuracy: 0.8925 - val_loss: 0.3184 - val_categorical_accuracy: 0.8701 - lr: 5.0000e-04\n",
      "Epoch 25/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.2593 - categorical_accuracy: 0.8730 - val_loss: 0.3433 - val_categorical_accuracy: 0.8701 - lr: 5.0000e-04\n",
      "Epoch 26/2000\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.2658 - categorical_accuracy: 0.8500\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.2685 - categorical_accuracy: 0.8502 - val_loss: 0.4248 - val_categorical_accuracy: 0.7922 - lr: 5.0000e-04\n",
      "Epoch 27/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.2334 - categorical_accuracy: 0.8990 - val_loss: 0.2790 - val_categorical_accuracy: 0.8701 - lr: 2.5000e-04\n",
      "Epoch 28/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.2067 - categorical_accuracy: 0.8990 - val_loss: 0.2729 - val_categorical_accuracy: 0.9091 - lr: 2.5000e-04\n",
      "Epoch 29/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.1921 - categorical_accuracy: 0.9218 - val_loss: 0.2812 - val_categorical_accuracy: 0.8961 - lr: 2.5000e-04\n",
      "Epoch 30/2000\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 0.1855 - categorical_accuracy: 0.9316 - val_loss: 0.2929 - val_categorical_accuracy: 0.8442 - lr: 2.5000e-04\n",
      "Epoch 31/2000\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.1957 - categorical_accuracy: 0.9055 - val_loss: 0.2659 - val_categorical_accuracy: 0.9221 - lr: 2.5000e-04\n",
      "Epoch 32/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.1816 - categorical_accuracy: 0.9186 - val_loss: 0.3365 - val_categorical_accuracy: 0.8701 - lr: 2.5000e-04\n",
      "Epoch 33/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.1867 - categorical_accuracy: 0.9121 - val_loss: 0.2556 - val_categorical_accuracy: 0.9351 - lr: 2.5000e-04\n",
      "Epoch 34/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.1693 - categorical_accuracy: 0.9349 - val_loss: 0.2473 - val_categorical_accuracy: 0.9221 - lr: 2.5000e-04\n",
      "Epoch 35/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.1754 - categorical_accuracy: 0.9088 - val_loss: 0.2463 - val_categorical_accuracy: 0.9351 - lr: 2.5000e-04\n",
      "Epoch 36/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.1533 - categorical_accuracy: 0.9349 - val_loss: 0.2490 - val_categorical_accuracy: 0.8831 - lr: 2.5000e-04\n",
      "Epoch 37/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.1550 - categorical_accuracy: 0.9381 - val_loss: 0.2857 - val_categorical_accuracy: 0.8442 - lr: 2.5000e-04\n",
      "Epoch 38/2000\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.1479 - categorical_accuracy: 0.9300\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.1459 - categorical_accuracy: 0.9316 - val_loss: 0.2734 - val_categorical_accuracy: 0.8961 - lr: 2.5000e-04\n",
      "Epoch 39/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.1271 - categorical_accuracy: 0.9642 - val_loss: 0.2235 - val_categorical_accuracy: 0.9481 - lr: 1.2500e-04\n",
      "Epoch 40/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.1202 - categorical_accuracy: 0.9642 - val_loss: 0.2509 - val_categorical_accuracy: 0.8831 - lr: 1.2500e-04\n",
      "Epoch 41/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.1188 - categorical_accuracy: 0.9577 - val_loss: 0.2304 - val_categorical_accuracy: 0.8961 - lr: 1.2500e-04\n",
      "Epoch 42/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.1105 - categorical_accuracy: 0.9577 - val_loss: 0.2161 - val_categorical_accuracy: 0.9610 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.1119 - categorical_accuracy: 0.9642 - val_loss: 0.2169 - val_categorical_accuracy: 0.9351 - lr: 1.2500e-04\n",
      "Epoch 44/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.1020 - categorical_accuracy: 0.9772 - val_loss: 0.2374 - val_categorical_accuracy: 0.8961 - lr: 1.2500e-04\n",
      "Epoch 45/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.1053 - categorical_accuracy: 0.9639\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.1069 - categorical_accuracy: 0.9609 - val_loss: 0.2223 - val_categorical_accuracy: 0.8961 - lr: 1.2500e-04\n",
      "Epoch 46/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0948 - categorical_accuracy: 0.9739 - val_loss: 0.2002 - val_categorical_accuracy: 0.9481 - lr: 6.2500e-05\n",
      "Epoch 47/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0874 - categorical_accuracy: 0.9739 - val_loss: 0.1980 - val_categorical_accuracy: 0.9610 - lr: 6.2500e-05\n",
      "Epoch 48/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0837 - categorical_accuracy: 0.9805 - val_loss: 0.2051 - val_categorical_accuracy: 0.9221 - lr: 6.2500e-05\n",
      "Epoch 49/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0816 - categorical_accuracy: 0.9772 - val_loss: 0.1907 - val_categorical_accuracy: 0.9610 - lr: 6.2500e-05\n",
      "Epoch 50/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0791 - categorical_accuracy: 0.9837 - val_loss: 0.2257 - val_categorical_accuracy: 0.8961 - lr: 6.2500e-05\n",
      "Epoch 51/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0785 - categorical_accuracy: 0.9837 - val_loss: 0.1982 - val_categorical_accuracy: 0.9740 - lr: 6.2500e-05\n",
      "Epoch 52/2000\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.0759 - categorical_accuracy: 0.9805\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0759 - categorical_accuracy: 0.9805 - val_loss: 0.1931 - val_categorical_accuracy: 0.9481 - lr: 6.2500e-05\n",
      "Epoch 53/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0684 - categorical_accuracy: 0.9902 - val_loss: 0.1923 - val_categorical_accuracy: 0.9610 - lr: 3.1250e-05\n",
      "Epoch 54/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0675 - categorical_accuracy: 0.9870 - val_loss: 0.2093 - val_categorical_accuracy: 0.9091 - lr: 3.1250e-05\n",
      "Epoch 55/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0716 - categorical_accuracy: 0.9870 - val_loss: 0.1899 - val_categorical_accuracy: 0.9610 - lr: 3.1250e-05\n",
      "Epoch 56/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0676 - categorical_accuracy: 0.9870 - val_loss: 0.1878 - val_categorical_accuracy: 0.9610 - lr: 3.1250e-05\n",
      "Epoch 57/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0668 - categorical_accuracy: 0.9870 - val_loss: 0.1935 - val_categorical_accuracy: 0.9351 - lr: 3.1250e-05\n",
      "Epoch 58/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0656 - categorical_accuracy: 0.9870 - val_loss: 0.1862 - val_categorical_accuracy: 0.9610 - lr: 3.1250e-05\n",
      "Epoch 59/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0665 - categorical_accuracy: 0.9837 - val_loss: 0.1946 - val_categorical_accuracy: 0.9351 - lr: 3.1250e-05\n",
      "Epoch 60/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0650 - categorical_accuracy: 0.9837 - val_loss: 0.1841 - val_categorical_accuracy: 0.9610 - lr: 3.1250e-05\n",
      "Epoch 61/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0634 - categorical_accuracy: 0.9870 - val_loss: 0.1853 - val_categorical_accuracy: 0.9610 - lr: 3.1250e-05\n",
      "Epoch 62/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0622 - categorical_accuracy: 0.9902 - val_loss: 0.1844 - val_categorical_accuracy: 0.9610 - lr: 3.1250e-05\n",
      "Epoch 63/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.0619 - categorical_accuracy: 0.9869\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0618 - categorical_accuracy: 0.9870 - val_loss: 0.1880 - val_categorical_accuracy: 0.9481 - lr: 3.1250e-05\n",
      "Epoch 64/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0599 - categorical_accuracy: 0.9870 - val_loss: 0.1836 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 65/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0594 - categorical_accuracy: 0.9902 - val_loss: 0.1832 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 66/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0587 - categorical_accuracy: 0.9902 - val_loss: 0.1837 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 67/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0583 - categorical_accuracy: 0.9870 - val_loss: 0.1833 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 68/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0579 - categorical_accuracy: 0.9870 - val_loss: 0.1831 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 69/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0580 - categorical_accuracy: 0.9902 - val_loss: 0.1836 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 70/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0574 - categorical_accuracy: 0.9870 - val_loss: 0.1822 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 71/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0557 - categorical_accuracy: 0.9902 - val_loss: 0.1837 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 72/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0560 - categorical_accuracy: 0.9902 - val_loss: 0.1828 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 73/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0556 - categorical_accuracy: 0.9902 - val_loss: 0.1819 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 74/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0555 - categorical_accuracy: 0.9902 - val_loss: 0.1822 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 75/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0552 - categorical_accuracy: 0.9902 - val_loss: 0.1817 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 76/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0558 - categorical_accuracy: 0.9902 - val_loss: 0.1808 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 77/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0543 - categorical_accuracy: 0.9902 - val_loss: 0.1805 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 78/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0538 - categorical_accuracy: 0.9902 - val_loss: 0.1799 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 79/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0549 - categorical_accuracy: 0.9902 - val_loss: 0.1787 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 80/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0526 - categorical_accuracy: 0.9902 - val_loss: 0.1795 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 81/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0527 - categorical_accuracy: 0.9902 - val_loss: 0.1799 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 82/2000\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0524 - categorical_accuracy: 0.9898\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0520 - categorical_accuracy: 0.9902 - val_loss: 0.1793 - val_categorical_accuracy: 0.9610 - lr: 1.5625e-05\n",
      "Epoch 83/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0509 - categorical_accuracy: 0.9902 - val_loss: 0.1789 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 84/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0507 - categorical_accuracy: 0.9902 - val_loss: 0.1784 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 85/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0509 - categorical_accuracy: 0.9902 - val_loss: 0.1783 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 86/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0508 - categorical_accuracy: 0.9902 - val_loss: 0.1780 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 87/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0500 - categorical_accuracy: 0.9902 - val_loss: 0.1779 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 88/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0500 - categorical_accuracy: 0.9902 - val_loss: 0.1780 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 89/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0498 - categorical_accuracy: 0.9902 - val_loss: 0.1774 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 90/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0490 - categorical_accuracy: 0.9902 - val_loss: 0.1786 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 91/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0500 - categorical_accuracy: 0.9902 - val_loss: 0.1773 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 92/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0490 - categorical_accuracy: 0.9902 - val_loss: 0.1770 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 93/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0489 - categorical_accuracy: 0.9902 - val_loss: 0.1766 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 94/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0487 - categorical_accuracy: 0.9902 - val_loss: 0.1764 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 95/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0488 - categorical_accuracy: 0.9902 - val_loss: 0.1767 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 96/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0486 - categorical_accuracy: 0.9902 - val_loss: 0.1768 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 97/2000\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.0485 - categorical_accuracy: 0.99 - 1s 18ms/step - loss: 0.0483 - categorical_accuracy: 0.9902 - val_loss: 0.1761 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 98/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0478 - categorical_accuracy: 0.9902 - val_loss: 0.1758 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 99/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0484 - categorical_accuracy: 0.9902 - val_loss: 0.1749 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 100/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0477 - categorical_accuracy: 0.9902 - val_loss: 0.1749 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 101/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0472 - categorical_accuracy: 0.9902 - val_loss: 0.1746 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 102/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0476 - categorical_accuracy: 0.9902 - val_loss: 0.1745 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 103/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0469 - categorical_accuracy: 0.9902 - val_loss: 0.1749 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 104/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0468 - categorical_accuracy: 0.9902 - val_loss: 0.1743 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 105/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0466 - categorical_accuracy: 0.9902 - val_loss: 0.1741 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 106/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0463 - categorical_accuracy: 0.9902 - val_loss: 0.1738 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 107/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0458 - categorical_accuracy: 0.9902 - val_loss: 0.1736 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 108/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0458 - categorical_accuracy: 0.9902 - val_loss: 0.1737 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 109/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0452 - categorical_accuracy: 0.9902 - val_loss: 0.1734 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 110/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0452 - categorical_accuracy: 0.9902 - val_loss: 0.1728 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 111/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0448 - categorical_accuracy: 0.9902 - val_loss: 0.1728 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 112/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0451 - categorical_accuracy: 0.9902 - val_loss: 0.1725 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 113/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0445 - categorical_accuracy: 0.9902 - val_loss: 0.1728 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 114/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0443 - categorical_accuracy: 0.9902 - val_loss: 0.1725 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 115/2000\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.0442 - categorical_accuracy: 0.9902\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "62/62 [==============================] - 2s 38ms/step - loss: 0.0442 - categorical_accuracy: 0.9902 - val_loss: 0.1725 - val_categorical_accuracy: 0.9610 - lr: 7.8125e-06\n",
      "Epoch 116/2000\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 0.0434 - categorical_accuracy: 0.9902 - val_loss: 0.1717 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 117/2000\n",
      "62/62 [==============================] - 2s 38ms/step - loss: 0.0434 - categorical_accuracy: 0.9902 - val_loss: 0.1714 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 118/2000\n",
      "62/62 [==============================] - 1s 22ms/step - loss: 0.0432 - categorical_accuracy: 0.9902 - val_loss: 0.1714 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 119/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0433 - categorical_accuracy: 0.9902 - val_loss: 0.1712 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 120/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0433 - categorical_accuracy: 0.9902 - val_loss: 0.1710 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 121/2000\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 0.0429 - categorical_accuracy: 0.9902 - val_loss: 0.1708 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 122/2000\n",
      "62/62 [==============================] - 2s 24ms/step - loss: 0.0428 - categorical_accuracy: 0.9902 - val_loss: 0.1708 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 123/2000\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.0427 - categorical_accuracy: 0.9902 - val_loss: 0.1708 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 124/2000\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0423 - categorical_accuracy: 0.9898\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0425 - categorical_accuracy: 0.9902 - val_loss: 0.1707 - val_categorical_accuracy: 0.9610 - lr: 3.9063e-06\n",
      "Epoch 125/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0423 - categorical_accuracy: 0.9902 - val_loss: 0.1705 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 126/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0424 - categorical_accuracy: 0.9902 - val_loss: 0.1705 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 127/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0424 - categorical_accuracy: 0.9902 - val_loss: 0.1705 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 128/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0421 - categorical_accuracy: 0.9902 - val_loss: 0.1703 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 129/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0421 - categorical_accuracy: 0.9902 - val_loss: 0.1704 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 130/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0421 - categorical_accuracy: 0.9902 - val_loss: 0.1701 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 131/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0419 - categorical_accuracy: 0.9902 - val_loss: 0.1701 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 132/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0419 - categorical_accuracy: 0.9902 - val_loss: 0.1700 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 133/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0418 - categorical_accuracy: 0.9902 - val_loss: 0.1699 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 134/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0418 - categorical_accuracy: 0.9902 - val_loss: 0.1700 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 135/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0417 - categorical_accuracy: 0.9902 - val_loss: 0.1698 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 136/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0417 - categorical_accuracy: 0.9902 - val_loss: 0.1697 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 137/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0415 - categorical_accuracy: 0.9902 - val_loss: 0.1696 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 138/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0414 - categorical_accuracy: 0.9902 - val_loss: 0.1697 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 139/2000\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.0420 - categorical_accuracy: 0.9900\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0415 - categorical_accuracy: 0.9902 - val_loss: 0.1696 - val_categorical_accuracy: 0.9610 - lr: 1.9531e-06\n",
      "Epoch 140/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0413 - categorical_accuracy: 0.9902 - val_loss: 0.1696 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 141/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0413 - categorical_accuracy: 0.9902 - val_loss: 0.1695 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 142/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0412 - categorical_accuracy: 0.9902 - val_loss: 0.1694 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 143/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0412 - categorical_accuracy: 0.9902 - val_loss: 0.1694 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 144/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0412 - categorical_accuracy: 0.9902 - val_loss: 0.1694 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 145/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0412 - categorical_accuracy: 0.9902 - val_loss: 0.1693 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 146/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0412 - categorical_accuracy: 0.9902 - val_loss: 0.1693 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 147/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0411 - categorical_accuracy: 0.9902 - val_loss: 0.1692 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 148/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0411 - categorical_accuracy: 0.9902 - val_loss: 0.1692 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 149/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0410 - categorical_accuracy: 0.9902 - val_loss: 0.1692 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 150/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0410 - categorical_accuracy: 0.9902 - val_loss: 0.1692 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 151/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.0339 - categorical_accuracy: 0.9934\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0410 - categorical_accuracy: 0.9902 - val_loss: 0.1691 - val_categorical_accuracy: 0.9610 - lr: 9.7656e-07\n",
      "Epoch 152/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0409 - categorical_accuracy: 0.9902 - val_loss: 0.1690 - val_categorical_accuracy: 0.9610 - lr: 4.8828e-07\n",
      "Epoch 153/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0408 - categorical_accuracy: 0.9902 - val_loss: 0.1690 - val_categorical_accuracy: 0.9610 - lr: 4.8828e-07\n",
      "Epoch 154/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0409 - categorical_accuracy: 0.9902 - val_loss: 0.1690 - val_categorical_accuracy: 0.9610 - lr: 4.8828e-07\n",
      "Epoch 155/2000\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0414 - categorical_accuracy: 0.9898\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0408 - categorical_accuracy: 0.9902 - val_loss: 0.1690 - val_categorical_accuracy: 0.9610 - lr: 4.8828e-07\n",
      "Epoch 156/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0408 - categorical_accuracy: 0.9902 - val_loss: 0.1690 - val_categorical_accuracy: 0.9610 - lr: 2.4414e-07\n",
      "Epoch 157/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0408 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 2.4414e-07\n",
      "Epoch 158/2000\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0410 - categorical_accuracy: 0.9898\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0408 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 2.4414e-07\n",
      "Epoch 159/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.2207e-07\n",
      "Epoch 160/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.2207e-07\n",
      "Epoch 161/2000\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9902\n",
      "Epoch 00161: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.2207e-07\n",
      "Epoch 162/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 6.1035e-08\n",
      "Epoch 163/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 6.1035e-08\n",
      "Epoch 164/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9902\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 6.1035e-08\n",
      "Epoch 165/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.0518e-08\n",
      "Epoch 166/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.0518e-08\n",
      "Epoch 167/2000\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0411 - categorical_accuracy: 0.9898\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.0518e-08\n",
      "Epoch 168/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.5259e-08\n",
      "Epoch 169/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.5259e-08\n",
      "Epoch 170/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9902\n",
      "Epoch 00170: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.5259e-08\n",
      "Epoch 171/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 7.6294e-09\n",
      "Epoch 172/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 7.6294e-09\n",
      "Epoch 173/2000\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9902\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 7.6294e-09\n",
      "Epoch 174/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.8147e-09\n",
      "Epoch 175/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.8147e-09\n",
      "Epoch 176/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.0410 - categorical_accuracy: 0.9902\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.8147e-09\n",
      "Epoch 177/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.9073e-09\n",
      "Epoch 178/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.9073e-09\n",
      "Epoch 179/2000\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.0412 - categorical_accuracy: 0.9900\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.9073e-09\n",
      "Epoch 180/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 9.5367e-10\n",
      "Epoch 181/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 9.5367e-10\n",
      "Epoch 182/2000\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9900\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 9.5367e-10\n",
      "Epoch 183/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 4.7684e-10\n",
      "Epoch 184/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 4.7684e-10\n",
      "Epoch 185/2000\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0421 - categorical_accuracy: 0.9898\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 4.7684e-10\n",
      "Epoch 186/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 2.3842e-10\n",
      "Epoch 187/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 2.3842e-10\n",
      "Epoch 188/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.0410 - categorical_accuracy: 0.9902\n",
      "Epoch 00188: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 2.3842e-10\n",
      "Epoch 189/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.1921e-10\n",
      "Epoch 190/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.1921e-10\n",
      "Epoch 191/2000\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9902\n",
      "Epoch 00191: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.1921e-10\n",
      "Epoch 192/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 5.9605e-11\n",
      "Epoch 193/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 5.9605e-11\n",
      "Epoch 194/2000\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.0416 - categorical_accuracy: 0.9900\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 5.9605e-11\n",
      "Epoch 195/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 2.9802e-11\n",
      "Epoch 196/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 2.9802e-11\n",
      "Epoch 197/2000\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0417 - categorical_accuracy: 0.9898\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 2.9802e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.4901e-11\n",
      "Epoch 199/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.4901e-11\n",
      "Epoch 200/2000\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0422 - categorical_accuracy: 0.9898\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.4901e-11\n",
      "Epoch 201/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 7.4506e-12\n",
      "Epoch 202/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 7.4506e-12\n",
      "Epoch 203/2000\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9902\n",
      "Epoch 00203: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 7.4506e-12\n",
      "Epoch 204/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.7253e-12\n",
      "Epoch 205/2000\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.7253e-12\n",
      "Epoch 206/2000\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.0415 - categorical_accuracy: 0.9900\n",
      "Epoch 00206: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 3.7253e-12\n",
      "Epoch 207/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.8626e-12\n",
      "Epoch 208/2000\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.8626e-12\n",
      "Epoch 209/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.0407 - categorical_accuracy: 0.9902\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 1.8626e-12\n",
      "Epoch 210/2000\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 9.3132e-13\n",
      "Epoch 211/2000\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.0410 - categorical_accuracy: 0.9902Restoring model weights from the end of the best epoch: 193.\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.0407 - categorical_accuracy: 0.9902 - val_loss: 0.1689 - val_categorical_accuracy: 0.9610 - lr: 9.3132e-13\n",
      "Epoch 00211: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f90dfa5eb0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, batch_size=5, validation_split=.2, shuffle=True,\n",
    "          callbacks=[tb_callback,\n",
    "                     EarlyStopping(patience=18, verbose=1, restore_best_weights=True),\n",
    "                     ReduceLROnPlateau(factor=.5, patience=3, verbose=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'street'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'street'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-412a1357352b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[87,  3],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[90,  0],\n",
       "        [ 2,  4]],\n",
       "\n",
       "       [[89,  1],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[90,  0],\n",
       "        [ 1,  5]],\n",
       "\n",
       "       [[82,  2],\n",
       "        [ 2, 10]],\n",
       "\n",
       "       [[83,  1],\n",
       "        [ 2, 10]],\n",
       "\n",
       "       [[81,  3],\n",
       "        [ 2, 10]],\n",
       "\n",
       "       [[83,  1],\n",
       "        [ 1, 11]],\n",
       "\n",
       "       [[90,  0],\n",
       "        [ 1,  5]],\n",
       "\n",
       "       [[90,  0],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[90,  0],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[90,  0],\n",
       "        [ 0,  6]]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8854166666666666"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45852625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = CategoricalCrossentropy()\n",
    "cc(y_test, model.predict(X_test)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    " \n",
    "import pyttsx3\n",
    "from gtts import gTTS  \n",
    "from playsound import playsound  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245),(16,117,245)]*4\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambulance 100%\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "Text_Speech=pyttsx3.init()\n",
    "\n",
    "dict1 =  {\"احتاج سيارة اسعاف\": \"i_need_amubalance\" }\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('./ambulance 8.avi')\n",
    "# cap = cv2.VideoCapture('./WhatsApp Video 2021-12-08 at 14.31.03.mp4')\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter('output2.avi', fourcc, 30.0, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),  \n",
    "                                                    int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: # end of video\n",
    "            break\n",
    "        \n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "#         print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        cv2.putText(image, str(len(sequence)), (30,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)], format(res[np.argmax(res)], '.0%'))\n",
    "            # convert text to speech\n",
    "            Text_Speech.say(actions[np.argmax(res)])\n",
    "        \n",
    "            Text_Speech.runAndWait()\n",
    "            obj = gTTS(text=\"شكراً لهذا المعسكر سدايا\", lang='ar', slow=False)  \n",
    "\n",
    "            #Here we are saving the transformed audio in a mp3 file named  \n",
    "            # exam.mp3  \n",
    "            obj.save(\"exam.mp3\")  \n",
    "\n",
    "            # Play the exam.mp3 file  \n",
    "            playsound(\"exam.mp3\")  \n",
    "            \n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "        #3. logic\n",
    "#             if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                \n",
    "\n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                sequence = [] # start new sequence\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-3:]\n",
    "\n",
    "            # Viz probabilities\n",
    "#             image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        text = \"احتاج مساعدة اين الشارع\"\n",
    "#         text = ' '.join([dict1.get(i, i) for i in sentence])\n",
    "        \n",
    "        reshaped_text = arabic_reshaper.reshape(text)    # correct its shape\n",
    "        bidi_text = get_display(reshaped_text)           # correct its direction\n",
    "        fontpath = \"arial.ttf\" # <== https://www.freefontspro.com/14454/arial.ttf  \n",
    "        font = ImageFont.truetype(fontpath, 32)\n",
    "        img_pil = Image.fromarray(image)\n",
    "        draw = ImageDraw.Draw(img_pil)\n",
    "        draw.text((100, 80),bidi_text, font = font)\n",
    "        img = np.array(img_pil)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', img)\n",
    "        out.write(image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=['a']\n",
    "text = ' '.join([dict1.get(i, i) for i in sentence])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imutils.video import FileVideoStream\n",
    "# from imutils.video import WebcamVideoStream\n",
    "# import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. New detection variables\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# predictions = []\n",
    "# threshold = 0.7\n",
    "# i = 0\n",
    "\n",
    "# # cap = cv2.VideoCapture('./ineed-9_hiz8v33F.mp4')\n",
    "# fvs = WebcamVideoStream(0).start()\n",
    "# # fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "# # out = cv2.VideoWriter('output2.avi', fourcc, 30.0, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),  \n",
    "# #                                                     int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "#     while True:\n",
    "            \n",
    "#         # Read feed\n",
    "#         frame = fvs.read()\n",
    "        \n",
    "#         if i == 30:\n",
    "#             i = 1\n",
    "#         else:\n",
    "#             i+=1\n",
    "#         cv2.putText(image, str(i), (50,5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "# #         print(results)\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_styled_landmarks(image, results)\n",
    "        \n",
    "#         # 2. Prediction logic\n",
    "#         keypoints = extract_keypoints(results)\n",
    "#         sequence.append(keypoints)\n",
    "#         sequence = sequence[-30:]\n",
    "        \n",
    "        \n",
    "#         if len(sequence) == 30:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "# #             print(actions[np.argmax(res)])\n",
    "#             predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "#         #3. logic\n",
    "#             if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "#                 if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "#                     if len(sentence) > 0: \n",
    "#                         if actions[np.argmax(res)] != sentence[-1]:\n",
    "#                             sentence.append(actions[np.argmax(res)])\n",
    "#                     else:\n",
    "#                         sentence.append(actions[np.argmax(res)])\n",
    "                    \n",
    "#                     sequence = [] # start new sequence\n",
    "\n",
    "#             if len(sentence) > 5: \n",
    "#                 sentence = sentence[-5:]\n",
    "\n",
    "#             # Viz probabilities\n",
    "#             image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "#         cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "#         cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "#                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "# #         print(' '.join(sentence), end=' ')\n",
    "        \n",
    "#         # Show to screen\n",
    "#         cv2.imshow('OpenCV Feed', image)\n",
    "# #         out.write(image)\n",
    "        \n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "    \n",
    "\n",
    "# #     out.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     fvs.stop()\n",
    "#     fvs.stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.destroyAllWindows()\n",
    "# fvs.stop()\n",
    "# fvs.stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
